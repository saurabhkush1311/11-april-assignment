{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?\n",
    "\n",
    "Q2. Why are ensemble techniques used in machine learning?\n",
    "\n",
    "Q3. What is bagging?\n",
    "\n",
    "Q4. What is boosting?\n",
    "\n",
    "\n",
    "Q5. What are the benefits of using ensemble techniques?\n",
    "\n",
    "Q6. Are ensemble techniques always better than individual models?\n",
    "\n",
    "Q7. How is the confidence interval calculated using bootstrap?\n",
    "\n",
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "\n",
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. An ensemble technique in machine learning involves combining multiple individual models to create a more powerful and accurate predictive model. It aims to leverage the diversity and collective wisdom of multiple models to improve overall performance.\n",
    "\n",
    "Q2. Ensemble techniques are used in machine learning for several reasons:\n",
    "   - Increased accuracy: Combining multiple models can help reduce errors and improve predictive accuracy.\n",
    "   - Robustness: Ensemble models are often more resistant to overfitting and generalization issues than single models.\n",
    "   - Handling complex problems: Ensemble methods can effectively handle complex patterns and relationships in data.\n",
    "   - Better generalization: Ensemble techniques can generalize well to new and unseen data by capturing different aspects of the underlying problem.\n",
    "\n",
    "Q3. Bagging, short for Bootstrap Aggregating, is an ensemble technique where multiple subsets of the original dataset are created through random sampling with replacement. Each subset is then used to train individual models, and the final prediction is obtained by aggregating the predictions of all individual models (e.g., by averaging for regression or voting for classification).\n",
    "\n",
    "Q4. Boosting is an ensemble technique that combines weak individual models (often referred to as \"weak learners\") sequentially to create a strong predictive model. Each weak learner is trained to correct the mistakes made by the previous models, with more emphasis placed on the instances that were incorrectly predicted. Boosting algorithms iteratively train models and assign weights to each instance, leading to a final model that is more accurate than its individual components.\n",
    "\n",
    "Q5. Benefits of using ensemble techniques include:\n",
    "   - Improved accuracy: Ensemble models often outperform individual models and provide more accurate predictions.\n",
    "   - Increased robustness: Ensemble methods reduce the risk of overfitting and handle noise and outliers better.\n",
    "   - Enhanced generalization: Ensemble techniques generalize well to unseen data, capturing different patterns and relationships.\n",
    "   - Better model stability: Ensemble models tend to be more stable since they rely on multiple models rather than a single model's prediction.\n",
    "\n",
    "Q6. While ensemble techniques can generally provide improved performance, they are not always guaranteed to be better than individual models. The effectiveness of ensemble methods depends on various factors such as the quality and diversity of the individual models, the nature of the problem, and the amount and quality of available data. In some cases, a single well-tuned model may perform better than an ensemble.\n",
    "\n",
    "Q7. The confidence interval using bootstrap can be calculated by estimating the mean of a statistic (e.g., the sample mean) based on multiple bootstrap samples and then computing the lower and upper percentiles of the distribution of those estimates. The range between these percentiles forms the confidence interval.\n",
    "\n",
    "Q8. Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic. The steps involved in bootstrap are as follows:\n",
    "   1. Random sampling with replacement: Create a large number of bootstrap samples by randomly selecting instances from the original sample, allowing for replacement.\n",
    "   2. Calculate the statistic: For each bootstrap sample, calculate the desired statistic (e.g., sample mean, standard deviation).\n",
    "   3. Estimate the sampling distribution: Obtain a distribution of the calculated statistic by collecting all the bootstrap sample statistics.\n",
    "   4. Compute the confidence interval: Determine the lower and upper percentiles of the distribution to establish the confidence interval.\n",
    "\n",
    "Q9. To estimate the 95% confidence interval for the population mean height using bootstrap, the researcher would follow these steps:\n",
    "   1. Collect a large number of bootstrap samples (e.g., 1000) from the original sample of 50 tree heights, allowing for replacement.\n",
    "   2. For each bootstrap sample, calculate the mean height.\n",
    "   3. Obtain the distribution of the bootstrap sample means.\n",
    "   4. Calculate the lower and upper percentiles of the distribution corresponding to the desired confidence level (95% in this case).\n",
    "   5. The range between these percentiles forms the 95% confidence interval for the population mean height."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
